#!/usr/bin/python3
# -*- coding: utf-8 -*-
#
#    Copyright (C) 2024 by YOUR NAME HERE
#
#    This file is part of RoboComp
#
#    RoboComp is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    RoboComp is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with RoboComp.  If not, see <http://www.gnu.org/licenses/>.
#

from PySide2.QtCore import QTimer
from PySide2.QtWidgets import QApplication
from rich.console import Console
from genericworker import *
import interfaces as ifaces

from huggingface_hub import hf_hub_download
from langchain_community.llms.llamacpp import LlamaCpp
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.vectorstores.chroma import Chroma
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

sys.path.append('/opt/robocomp/lib')
console = Console(highlight=False)


# If RoboComp was compiled with Python bindings you can use InnerModel in Python
# import librobocomp_qmat
# import librobocomp_osgviewer
# import librobocomp_innermodel


class SpecificWorker(GenericWorker):
    def __init__(self, proxy_map, startup_check=False):
        """
        Initializes a `SpecificWorker` object by creating a proxy map, setting the
        `Period` attribute to 10, and connecting a timeout event to call the
        `compute` method when the timer expires. It also initializes an LLM and
        conversation history.

        Args:
            proxy_map (`object`.): mapping between original functions and their
                corresponding proxies, which are then initialized by the `SpecificWorker`.
                
                	* `proxy_map`: a mapping containing information about the worker
                and its proxy configuration.
            startup_check (bool): 10-second startup check that the function performs
                when it is initialized.

        """
        super(SpecificWorker, self).__init__(proxy_map)
        self.Period = 10
        if startup_check:
            self.startup_check()
        else:
            self.timer.timeout.connect(self.compute)
            self.timer.start(self.Period)

        # Initialize LLM and conversation history
        self.init_llm()

    def __del__(self):
        """Destructor"""

    def setParams(self, params):
        # try:
        #	self.innermodel = InnerModel(params["InnerModelPath"])
        # except:
        #	traceback.print_exc()
        #	print("Error reading config params")
        return True


    @QtCore.Slot()
    def compute(self):
        """
        Generates high-quality documentation for given code using passive voice,
        formal tone, and without personal statements or references to the question.
        It does not repeat the question, use first-person pronouns, or include summaries.

        Returns:
            undefined: a response generated by the LLM model based on the input provided.

        """
        print('SpecificWorker.compute...')

        # testing LLM
        print(self.LLM_generateResponse(input("Talk to me:\n")))

        return True

    def startup_check(self):
        QTimer.singleShot(200, QApplication.instance().quit)

    # =============== Methods for Component Implements ==================
    # ===================================================================


    def format_docs(self, docs):
        """
        Combines the page content of multiple code documentation files into a
        single string, separating each document with a newline character (-).

        Args:
            docs (list): content of documentation pages to be compiled into a
                string of hyphens and page contents.

        Returns:
            undefined: a list of page contents from given documentation documents,
            separated by newlines.

        """
        text = ""
        for d in docs:
            text += f"- {d.page_content}\n"

        return text
    
    def init_llm(self):
        # Download model from Huggingface
        """
        Loads a language model (`self.llm`) and creates a prompt template (`prompt`)
        for a chatbot, using the given embedding model (`embeddings_model`) to
        generate true/false questions about any topic mentioned by the user.

        """
        model_path = hf_hub_download(
            repo_id="TheBloke/Mistral-7B-OpenOrca-GGUF",
            filename="mistral-7b-openorca.Q8_0.gguf",
            force_download=False
        )

        # Load the model
        self.llm = LlamaCpp(
            model_path=model_path,
            stop=["<|im_end|>"],
            n_gpu_layers=-1,
            n_ctx=2048*4,
            max_tokens=2048,
            temperature=0.3, 
            streaming=True, 
            top_p=0.8
        )

        model_name = "mixedbread-ai/mxbai-embed-large-v1"
        model_kwargs = {"device": "cpu"}
        encode_kwargs = {"normalize_embeddings": True}

        embeddings_model = HuggingFaceBgeEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )

        # Initialize conversation history
        self.db = Chroma(embedding_function=embeddings_model)

        retriever = self.db.as_retriever()

        self.db.add_texts([""])

        # create the prompt template
        template = """<|im_start|>system: you speak fluent Spanish and you are able to generate good true/false questions about the topic the user mentions. 
        {system_message}<|im_end|>
        <|im_start|>user
        {prompt}<|im_end|>
        <|im_start|>assistant
        """

        prompt = PromptTemplate.from_template(template)

        # create the chain
        output_parser = StrOutputParser()

        setup_and_retrieval = RunnableParallel(
            {"system_message": retriever | self.format_docs, "prompt": RunnablePassthrough()}
        )

        self.chain = setup_and_retrieval | prompt | self.llm | output_parser

    ############################
    # GENERATE RESPONSE
    ############################

    def LLM_generateResponse(self, user_response):
        # Interacting with model
        """
        Takes a user input and generates a corresponding response from a large
        language model (LLM) database, adding the input and LLM response to the
        chroma database for future reference.

        Args:
            user_response (str): text input provided by the user.

        Returns:
            undefined: a high-quality document generated using the given user
            response as input.

        """
        llm_response = self.chain.invoke(user_response)
        print("Assistant's response:")

        # updating chroma database
        self.db.add_texts([" - User: " + user_response + " - Assistant: " + llm_response])

        return llm_response
    # ===================================================================
    # ===================================================================

